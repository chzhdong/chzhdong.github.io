---
title: Llumnix Dynamic Scheduling for Large Language Model Serving
published: 2024-01-14
description: 'The paper reading note for Llumnix'
image: ''
tags: [Scheduling]
category: 'Paper'
draft: false 
lang: 'zh_CN'

---

## 论文概述

论文标题：**Llumnix: Dynamic Scheduling for Large Language Model Serving**

核心工作：对于大模型推理服务中，论文构建了Llumnix大模型推理系统实现对**异构并不可预测**的服务请求在多个模型的实例中进行**runtime rescheduling**。类似于操作系统中CPU核心中的上下文切换，Llumnix通过对请求的重新调度，实现了负载均衡、隔离、缓解资源片段、处理请求优先级以及满足Service-Level Objects (SLO)。为了实现请求的重新调度，Llumnix实现了一个实时迁移机制，充分利用动态调度策略，使得多个重新调度的场景能够统一。论文工作的源代码：[Llumnix](Llumnix: Dynamic Scheduling for Large Language Model Serving)

## 论文背景及存在的问题

目前的大模型推理系统由一个大模型的多种实例部署在GPU集群上提供支持。这个系统主要包括一个调度器和一个推理引擎，即服务请求会被调度器分发到一个模型实例中进行推理，在通过推理引擎进行执行并返回推理结果。

通过了解上述推理流程，发现其两个重要特性：

1. workload heterogeneity，即对于大模型而言，它的功能性是普适的，但对于不同的应用场景和用户，他的具体服务请求又是不相同的。也就是不同的服务请求，其输入长度，输出长度，期待的延迟都是不相同的。
2. execution unpredictability，大模型对于服务请求的输出长度是不可知的，那么也就是其服务延迟以及对于所需要的显存也是不可知的。

基于上述两种特性，目前的大模型推理系统主要面临以下挑战：

1. Isolation，由于服务请求的不可知会使得其内存使用的需求不断增长；而系统中的显存又是有限的，即导致不同服务请求之间的显存争用而引起性能干扰。
1. Fragmentation，由于变化的服务请求长度以及显存的需要不可避免地导致内存片段；对于运行的服务请求，系统会考虑负载均衡来分配不同实例的服务请求以减少延迟和抢占。但对于需要大块显存的服务请求而言，复杂均衡所导致的内存片段无法满足推理的需求，导致较长的阻塞延迟。
1. Priority，由于大模型中不同任务的特性不同，所以会影响它们所要求的延迟目标；例如聊天助理需要实时的反馈交互信息，而文章总结等工作就不太在意延迟。

在大模型推理工作中，不同应用的服务请求面临不同的需求，即序列长度和预期延迟。对于大模型推理过程中，自回归生成是以流的形式进行反馈，即**prefill**和**decode**的延迟都是用户可感知并影响用户体验。在目前的大模型推理中，**Batching**的工作方式主要为continous batching，也就是完成的服务请求可以随时加入或离开当前请求；GPU显存的管理主要是基于PagedAttention机制来实现和管理。

## 论文的主要工作

### 重新调度服务请求

1. 满足负载均衡以减少抢占和干扰
2. 去除片段以缓解排队延迟
3. 提供请求优先级以提供隔离
4. 饱和或耗尽实例以满足自动缩放

![reschedule](./assets/rescheduling scenarios.png)

### 实时迁移机制

Llumnix引入几乎无需停机时间的请求调度机制；对于任意长度都是一个常量，其是通过仔细地协调内存传输和计算来隐藏其所需的费用。

### 统一的重调度场景

为了统一请求重新调度场景的工作，Llumnix引入了**Virtual Usage**概念，通过定义GPU显存的使用规则以现实用简单的复杂均衡方法实现多场景的统一调度任务。

## 论文工作动机

### 无法预测的显存需求和抢占

由于大模型推理的特性，服务请求的显存需求不断增长，可能导致显存不足而需要对当前的部分请求进行抢占以释放显存；这导致其抢占的排队时延以及重新计算KV Cache的计算，从而影响大模型推理的性能。

### 请求间性能干扰

同一个批次间由于资源的竞争，即GPU计算和显存带宽的争用而导致服务请求之间的性能干扰。

### 显存片段

为了缓解服务请求之间的性能干扰，通过负载均衡将请求在实例中进行统一分配；然而，该方法可能会影响到显存片段，也就是未分配的显存。对于**decode**阶段，pagedattention方法有效地解决了外部的显存片段；但对于**prefill**阶段，需要大量的显存资源来初始化服务请求，由于负载均衡可能导致无法找到满足要求的实例，使得产生较大的排队时延。

### 请求优先级

显而易见，即当前的推理系统对于所有的服务请求一视同仁，而不同的应用实际上对推理延迟有着不同的要求，需要一个系统性的方法来满足每个服务请求所需的延迟目标。